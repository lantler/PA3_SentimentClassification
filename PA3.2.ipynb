{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f201ff7",
   "metadata": {},
   "source": [
    "#### Authors: Group 2\n",
    " - Leah Antler\n",
    " - Anna Fenn\n",
    " - June Huck\n",
    " - Vitaliy Kishchenko\n",
    "\n",
    "#### Submitted 06/28/2024\n",
    "\n",
    "#### Course #: AIT-526-002\n",
    "\n",
    "## QA Program Explanation\n",
    "\n",
    "### The Problem to be solved:\n",
    "This program is a Naive Bayes classifier that aims to classify the sentiment of tweets as positive or negative. This is a relevant problem as it automates the task of determining the sentiment of large amounts of information.\n",
    "\n",
    "### Examples, usage instructions:\n",
    "Our program makes use of tweets with labeled classes in individual text files. To classify a single tweet, the user can load their tweet in a text file. For example, if the input is: \"@SouthwestAir I would appreciate that.  Thank you.\"\n",
    "\n",
    "The program will first tokenize and stem it to be: ['@', 'southwestair', 'i', 'would', 'appreci', 'that', '.', 'thank', 'you']\n",
    "\n",
    "Then, count the number of times each word appears in the positive and negative corpus.\n",
    "\n",
    "And then calculate the likelihood of each word being positive or negative using the equation. Finally it will calculate the overall probability of the tweet being positive or negative and assign the class by the higher probability.\n",
    "\n",
    "\n",
    "### Algorithm:\n",
    "1. Preprocess the tweet data by lowercasing, tokenizing, decoding emojis, and stemming (optional).\n",
    "2. Create a vocabulary of all the words in the positive and negative classes (with and without stemming).\n",
    "3. Create the count vectors, which stores the count of each word in each tweet in the positive and negative classes.\n",
    "4. Create the binary vectors, which store the unique instances of each word in each tweet in the positive and negative classes.\n",
    "5. Predict using the Naive Bayes classifier by assigning positive or negative based on which has a higher likelihood.\n",
    "6. Evaluate performance by computing confusion matrices for each version of the classifier.\n",
    "\n",
    "\n",
    "### Additional Description:\n",
    "Bonus answer question (tf x idf) was not answered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a2e9d-5fa4-4b11-aed9-d3227721feea",
   "metadata": {},
   "source": [
    "### Import data from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4da4ca-649d-4a3e-b623-e62e73f4c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c8c4e9-7613-4155-9808-13a7fa06f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data(base_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    os.chdir(base_path)\n",
    "    \n",
    "    for sentiment in ['positive', 'negative']:\n",
    "        os.chdir(sentiment)\n",
    "        files = glob.glob('*.txt')\n",
    "        for file_name in files:\n",
    "            with open(file_name, 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                data.append(content)\n",
    "                labels.append(1 if sentiment == 'positive' else 0)\n",
    "        \n",
    "        os.chdir('..')\n",
    "\n",
    "    os.chdir('..')\n",
    "    return data, labels\n",
    "\n",
    "base_path = '.'\n",
    "train_data, train_labels = load_data(base_path + '/train')\n",
    "test_data, test_labels = load_data(base_path + '/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ceac39-6497-4bc6-ace4-0743fe4d0074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@SouthwestAir I would appreciate that.  Thank you.', '@USAirways thank you very much.', \"@JetBlue I'm all set. About to fly. Not bad for a first date with a giant metal bird machine. She even brought snacks.\", '@SouthwestAir I got a flight at 11:55am on Thursday but looking for something tomorrow anything available?', \"@AmericanAir you're my early frontrunner for best airline! #oscars2016\"]\n",
      "['@united maybe on my return trip üëç', \"@AmericanAir no kidding! Gonna take some beating on the apron... And there are some good lookin' planes out there!\", '@AmericanAir thanks', '@AmericanAir many trips coming up!  I will see you soon üòÉ', '@JetBlue Thank you guys! Brilliant customer service']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:5])\n",
    "print(test_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63407b9-118d-4475-ae44-5f5d367ccabc",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54fd3a-535f-4601-9a74-1b6bc9585df2",
   "metadata": {},
   "source": [
    "#### Lowercase capitals at the beginning of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae12b30e-9b4b-4053-be4b-2abf4694f0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lowercase_caps(text):\n",
    "\n",
    "    def lower_first_letter(match):\n",
    "        return match.group(1) + match.group(2).lower() + match.group(3)\n",
    "\n",
    "    pattern = r'(\\A|\\.\\s+|\\?\\s+|!\\s+)([A-Z])(\\w*)'\n",
    "    result = re.sub(pattern, lower_first_letter, text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d324e1c1-d687-491a-815b-46568f74fe02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lower = [lowercase_caps(tweet) for tweet in train_data]\n",
    "test_lower = [lowercase_caps(tweet) for tweet in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f9768-f1e0-46fc-b90b-f5c292444077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2bd588c-3450-440f-a15a-8402f2dbf15b",
   "metadata": {},
   "source": [
    "#### Tokenize, build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70fc6b74-3cd7-4c34-a57c-c0c963faac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demojize_text(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "train_lower_demoj = [demojize_text(tweet) for tweet in train_lower]\n",
    "test_lower_demoj = [demojize_text(tweet) for tweet in test_lower]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341aec10-0560-40d2-88b1-60257d8936c9",
   "metadata": {},
   "source": [
    "#### Create 2 versions of V: with stemming and without stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f318f889-b6b2-4003-a612-dd4c0c4962f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def build_vocabulary(texts, use_stemming=False):\n",
    "    \"\"\" Build vocabulary with optional stemming \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenize(text)\n",
    "        if use_stemming:\n",
    "            tokens = stem_tokens(tokens, stemmer)\n",
    "        vocabulary.update(tokens)\n",
    "\n",
    "    return list(vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3cca02a-3d03-4c1b-9724-587916c1e374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V without Stemming: ['@', 'SouthwestAir', 'I', 'would', 'appreciate', 'that', '.', 'thank', 'you', 'USAirways', 'very', 'much', 'JetBlue', \"'m\", 'all', 'set', 'about', 'to', 'fly', 'not', 'bad', 'for', 'a', 'first', 'date', 'with', 'giant', 'metal', 'bird', 'machine', 'she', 'even', 'brought', 'snacks', 'got', 'flight', 'at', '11:55am', 'on', 'Thursday', 'but', 'looking', 'something', 'tomorrow', 'anything', 'available', '?', 'AmericanAir', \"'re\", 'my', 'early', 'frontrunner', 'best', 'airline', '!', '#', 'oscars2016', 'RedCarpet', 'Southwest', 'Companion', 'Pass', 'be', 'great', 'major', 'issues', 'getting', 'out', 'of', 'Boston', 'your', 'crew', 'has', 'been', 'exceptional', 'let', \"'s\", 'see', 'how', 'things', 'roll', 'in', 'Philly', 'thanks', 'i', 'prompt', 'response', 'united', 'such', 'relaxing', 'space', 'drink', 'before', '(', 'United', 'Global', 'First', 'Lounge', ')', 'https', ':']\n",
      "V with Stemming: ['@', 'southwestair', 'i', 'would', 'appreci', 'that', '.', 'thank', 'you', 'usairway', 'veri', 'much', 'jetblu', \"'m\", 'all', 'set', 'about', 'to', 'fli', 'not', 'bad', 'for', 'a', 'first', 'date', 'with', 'giant', 'metal', 'bird', 'machin', 'she', 'even', 'brought', 'snack', 'got', 'flight', 'at', '11:55am', 'on', 'thursday', 'but', 'look', 'someth', 'tomorrow', 'anyth', 'avail', '?', 'americanair', \"'re\", 'my', 'earli', 'frontrunn', 'best', 'airlin', '!', '#', 'oscars2016', 'redcarpet', 'southwest', 'companion', 'pass', 'be', 'great', 'major', 'issu', 'get', 'out', 'of', 'boston', 'your', 'crew', 'ha', 'been', 'except', 'let', \"'s\", 'see', 'how', 'thing', 'roll', 'in', 'philli', 'prompt', 'respons', 'unit', 'such', 'relax', 'space', 'drink', 'befor', '(', 'global', 'loung', ')', 'http', ':', '//t.co/j4cj0lrf2d', '//t.co/dtlguq1kak', 'ok.', 'just']\n"
     ]
    }
   ],
   "source": [
    "v_stemming = build_vocabulary(train_lower_demoj, use_stemming=False)\n",
    "v_no_stemming = build_vocabulary(train_lower_demoj, use_stemming=True)\n",
    "\n",
    "print(\"V without Stemming:\", v_stemming[:100])\n",
    "print(\"V with Stemming:\", v_no_stemming[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c181a7b-cfab-410c-94d0-40404cb60cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b637d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SouthwestAir I would appreciate that.  Thank ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USAirways thank you very much.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JetBlue I'm all set. About to fly. Not bad fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SouthwestAir I got a flight at 11:55am on Thu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@AmericanAir you're my early frontrunner for b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  @SouthwestAir I would appreciate that.  Thank ...      1\n",
       "1                    @USAirways thank you very much.      1\n",
       "2  @JetBlue I'm all set. About to fly. Not bad fo...      1\n",
       "3  @SouthwestAir I got a flight at 11:55am on Thu...      1\n",
       "4  @AmericanAir you're my early frontrunner for b...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary of lists \n",
    "dictionary = {'text': train_data, 'label': train_labels} \n",
    "   \n",
    "df = pd.DataFrame(dictionary)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfcdb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of positive words\n",
    "positive_words_stemmed = []\n",
    "positive_words_nostem = []\n",
    "\n",
    "#list of all words where label = 1\n",
    "for index, row in df[df['label'] == 1].iterrows():\n",
    "    lower_tweet = lowercase_caps(row['text'])\n",
    "    demoj_tweet = demojize_text(lower_tweet)\n",
    "    tokenized_tweet = word_tokenize(demoj_tweet)\n",
    "    \n",
    "    positive_words_stemmed.extend(build_vocabulary(tokenized_tweet, use_stemming=True))\n",
    "    positive_words_nostem.extend(build_vocabulary(tokenized_tweet, use_stemming=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dae351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of negative words\n",
    "negative_words_stemmed = []\n",
    "negative_words_nostem = []\n",
    "\n",
    "#list of all words where label= 0\n",
    "for index, row in df[df['label'] == 0].iterrows():\n",
    "    lower_tweet = lowercase_caps(row['text'])\n",
    "    demoj_tweet = demojize_text(lower_tweet)\n",
    "    tokenized_tweet = word_tokenize(demoj_tweet)\n",
    "    \n",
    "    negative_words_stemmed.extend(build_vocabulary(tokenized_tweet, use_stemming=True))\n",
    "    negative_words_nostem.extend(build_vocabulary(tokenized_tweet, use_stemming=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f28e6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the combined dictionaries\n",
    "def create_combined_dict(pos_words, neg_words):\n",
    "    positive_words= set(pos_words)\n",
    "    remove_list =[]\n",
    "    negative_words= set(neg_words)\n",
    "\n",
    "    #Removal of Links in word sets\n",
    "    for word in positive_words:\n",
    "        if \"//t.co\" in word:\n",
    "            remove_list.append(word)\n",
    "\n",
    "    for word in remove_list:\n",
    "        positive_words.remove(word)\n",
    "    remove_list =[]\n",
    "    for word in negative_words:\n",
    "        if \"//t.co\" in word:\n",
    "            remove_list.append(word)\n",
    "\n",
    "    for word in remove_list:\n",
    "        negative_words.remove(word)\n",
    "\n",
    "    #create the dictionary\n",
    "    positive_words_dict = {word: 1 for word in positive_words}\n",
    "    negative_words_dict = {word: 0 for word in negative_words}\n",
    "\n",
    "    #combine both dictionaries\n",
    "    combined_dict = {**positive_words_dict, **negative_words_dict}\n",
    "    return combined_dict\n",
    "\n",
    "combined_dict_stemmed = create_combined_dict(positive_words_stemmed, negative_words_stemmed)\n",
    "combined_dict_nostem = create_combined_dict(positive_words_nostem, negative_words_nostem)\n",
    "# print(combined_dict)\n",
    "#print(positive_words_nostem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16204831-47e1-4743-87ee-7cef2c225b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Count Vectorizer\n",
    "#Supply data set of just tweets, pass in dataframe\n",
    "def CountVectorizer(data, use_stemming=False):\n",
    "\n",
    "    #Incase of stem usage\n",
    "    stemmer = PorterStemmer()\n",
    "    # Create a dictionary to store counts\n",
    "    counts = {1: defaultdict(int), 0: defaultdict(int)}\n",
    "\n",
    "    # Process the text\n",
    "    for index, row in data.iterrows():\n",
    "        lower_tweet = lowercase_caps(row['text'])\n",
    "        demoj_tweet = demojize_text(lower_tweet)\n",
    "        tokenized_tweet = word_tokenize(demoj_tweet)\n",
    "        if use_stemming:\n",
    "            tokenized_tweet = stem_tokens(tokenized_tweet, stemmer)\n",
    "    \n",
    "        # Update counts for the respective label\n",
    "        for word in tokenized_tweet:\n",
    "            #Ignores links\n",
    "            if \"//t.co\" not in word:\n",
    "                counts[row['label']][word] += 1\n",
    "    return counts\n",
    "\n",
    "MultiCountsStem = CountVectorizer(df, use_stemming=True)\n",
    "MultiCountsNoStem = CountVectorizer(df, use_stemming=False)\n",
    "# Display the counts\n",
    "#print(\"Positive counts:\", dict(Multicountsstem[1]))\n",
    "#print(\"Positive counts:\", dict(Multicountsnostem[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017cf61e-3d51-4da0-a4d9-d6723094304d",
   "metadata": {},
   "source": [
    "### Create Binary Vectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "450f3951-f86e-4195-a7a8-62c3d16b9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryVectorizer(data, labels, use_stemming=False):\n",
    "    #Incase of stem usage\n",
    "    stemmer = PorterStemmer()\n",
    "    # initialize dictionaries (pos and neg tweets)\n",
    "    counts = {\n",
    "        1: defaultdict(int),\n",
    "        0: defaultdict(int) \n",
    "    }\n",
    "    \n",
    "        # clean and tokenize each tweet\n",
    "    for text, label in zip(data, labels):\n",
    "\n",
    "        text = lowercase_caps(text)\n",
    "        text = demojize_text(text)\n",
    "        tokens = word_tokenize(text)\n",
    "        if use_stemming:\n",
    "            tokens = stem_tokens(tokens, stemmer)\n",
    "\n",
    "        # use a set to track unique words\n",
    "        unique_words = set(tokens)\n",
    "        for word in unique_words:\n",
    "            #Ignores links\n",
    "            if \"//t.co\" not in word:\n",
    "                counts[label][word] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "BinaryCountsStem = BinaryVectorizer(train_data, train_labels, use_stemming=True)\n",
    "BinaryCountsNoStem = BinaryVectorizer(train_data, train_labels, use_stemming=False)\n",
    "#print(BinaryCountsStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d66c843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Tweets: 1181\n",
      "Number of Negative Tweets: 3000\n",
      "Total Number of Tweets: 4181\n"
     ]
    }
   ],
   "source": [
    "#used to calculate the priors\n",
    "num_positive_tweets= len(df[df['label'] == 1])\n",
    "num_negative_tweets= len(df[df['label'] == 0])\n",
    "total_train= len(train_data)\n",
    "\n",
    "print(f\"Number of Positive Tweets: {num_positive_tweets}\")\n",
    "print(f\"Number of Negative Tweets: {num_negative_tweets}\")\n",
    "print(f\"Total Number of Tweets: {total_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e1b22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihoods(vector, num_negative_words, num_positive_words, total_vocab):\n",
    "    #iteerate over each row in the dataframe\n",
    "    likelihoods = {\n",
    "    0: defaultdict(int),  # Likelihoods for word to show up in negative tweets\n",
    "    1: defaultdict(int)   # Likelihoods for word to show up in positive tweets\n",
    "}\n",
    "    #get probabilities for words in corpus being in a negative tweet\n",
    "    for word in dict(vector[0]):\n",
    "        likelihoods[0][word] = vector[0][word]/(num_negative_words + total_vocab)\n",
    "    for word in dict(vector[1]):\n",
    "        likelihoods[1][word] = vector[1][word]/(num_positive_words + total_vocab)\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "344dbbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Variables for Likelihood usage\n",
    "#ms = Count/Multi + Stem, mn = Count/Multi + No Stem\n",
    "#bs = Binary + Stem, bn = Binary + No Stem\n",
    "num_negative_words_ms= len(dict(MultiCountsStem[0]))\n",
    "num_positive_words_ms= len(dict(MultiCountsStem[1]))\n",
    "num_negative_words_mn= len(dict(MultiCountsNoStem[0]))\n",
    "num_positive_words_mn= len(dict(MultiCountsNoStem[1]))\n",
    "num_negative_words_bs= len(dict(BinaryCountsStem[0]))\n",
    "num_positive_words_bs= len(dict(BinaryCountsStem[1]))\n",
    "num_negative_words_bn= len(dict(BinaryCountsNoStem[0]))\n",
    "num_positive_words_bn= len(dict(BinaryCountsNoStem[1]))\n",
    "total_vocab_stemmed= len(combined_dict_stemmed)\n",
    "total_vocab_nostem= len(combined_dict_nostem)\n",
    "\n",
    "likelihoodMS = get_likelihoods(MultiCountsStem, num_negative_words_ms, num_positive_words_ms, total_vocab_stemmed)\n",
    "likelihoodMN = get_likelihoods(MultiCountsNoStem, num_negative_words_mn, num_positive_words_mn, total_vocab_nostem)\n",
    "likelihoodBS = get_likelihoods(BinaryCountsStem, num_negative_words_bs, num_positive_words_bs, total_vocab_stemmed)\n",
    "likelihoodBN = get_likelihoods(BinaryCountsNoStem, num_negative_words_bn, num_positive_words_bn, total_vocab_nostem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cc8a780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #print positive likelihoods\n",
    "# print(\"Likelihoods for positive class:\", dict(likelihoodMS[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2037da4-f238-4c17-8274-5edd7a9e7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print negative likelihoods\n",
    "#print(\"Likelihoods for positive class:\", dict(likelihoodMS[0]))\n",
    "#print (total_vocab_nostem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154abca3-c30e-4766-8796-f0de4e505120",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predicting using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc198d83-53e4-4f85-8b51-fba8282ccd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def handle_negations(text):\n",
    "    negations = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "    tokens = word_tokenize(text)\n",
    "    negated_tokens = []\n",
    "    negate = False\n",
    "    for token in tokens:\n",
    "        if token in negations:\n",
    "            negate = True\n",
    "            negated_tokens.append(token)\n",
    "        elif negate:\n",
    "            negated_tokens.append(\"NOT_\" + token)\n",
    "            negate = False\n",
    "        else:\n",
    "            negated_tokens.append(token)\n",
    "    return negated_tokens\n",
    "\n",
    "def predict(tweet, likelihoods, prior_positive, prior_negative, total_vocab, num_positive_tweets, num_negative_tweets, use_stemming=False):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokenized_tweet = handle_negations(demojize_text(lowercase_caps(tweet)))\n",
    "    if use_stemming:\n",
    "        tokenized_tweet = stem_tokens(tokenized_tweet, stemmer)\n",
    "    \n",
    "    log_prob_positive = prior_positive\n",
    "    log_prob_negative = prior_negative\n",
    "\n",
    "#add laplace smoothing\n",
    "    for word in tokenized_tweet:\n",
    "        log_prob_positive += math.log((likelihoods[1].get(word, 0) + 1) / (num_positive_tweets + total_vocab))\n",
    "        log_prob_negative += math.log((likelihoods[0].get(word, 0) + 1) / (num_negative_tweets + total_vocab))\n",
    "    \n",
    "    return 1 if log_prob_positive > log_prob_negative else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b009de7-978c-4676-9922-07467e23936f",
   "metadata": {},
   "source": [
    "### Calculating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca1c641-7c5e-498b-b8dc-b4e7f8ebac08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                  Predicted Positive   Predicted Negative\n",
      "Actual Positive                   1092                             90\n",
      "Actual Negative                   2974                             26\n",
      "\n",
      "Performance Metrics:\n",
      "Recall:    0.9239\n",
      "Precision: 0.2686\n",
      "Accuracy:  0.2673\n",
      "F1 Score:  0.4162\n"
     ]
    }
   ],
   "source": [
    "def calculate_confusion_matrix(predicted_labels, actual_labels):\n",
    "    tp = fp = tn = fn = 0\n",
    "\n",
    "    for pred, actual in zip(predicted_labels, actual_labels):\n",
    "        if pred == 1 and actual == 1:\n",
    "            tp += 1\n",
    "        elif pred == 1 and actual == 0:\n",
    "            fp += 1\n",
    "        elif pred == 0 and actual == 0:\n",
    "            tn += 1\n",
    "        elif pred == 0 and actual == 1:\n",
    "            fn += 1\n",
    "\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def calculate_performance_metrics(tp, fp, tn, fn):\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return recall, precision, accuracy, f1_score\n",
    "\n",
    "def evaluate_model(test_data, test_labels, likelihoods, prior_positive, prior_negative, total_vocab, num_positive_tweets, num_negative_tweets, use_stemming=False):\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for tweet in test_data:\n",
    "        predicted_class = predict(tweet, likelihoods, prior_positive, prior_negative, total_vocab, num_positive_tweets, num_negative_tweets, use_stemming)\n",
    "        predicted_labels.append(predicted_class)\n",
    "    \n",
    "    tp, fp, tn, fn = calculate_confusion_matrix(predicted_labels, test_labels)\n",
    "    recall, precision, accuracy, f1_score = calculate_performance_metrics(tp, fp, tn, fn)\n",
    "    \n",
    "    return tp, fp, tn, fn, recall, precision, accuracy, f1_score\n",
    "\n",
    "import logging\n",
    "\n",
    "# set up log file\n",
    "logging.basicConfig(filename='output.txt', level=logging.INFO, format='%(message)s')\n",
    "\n",
    "#log results\n",
    "def log_results(tp, fp, tn, fn, recall, precision, accuracy, f1_score, classifiertype):\n",
    "    logging.info(classifiertype)\n",
    "    logging.info(\"Confusion Matrix:\")\n",
    "    logging.info(f\"                  Predicted Positive   Predicted Negative\")\n",
    "    logging.info(f\"Actual Positive   {tp:20}           {fn:20}\")\n",
    "    logging.info(f\"Actual Negative   {fp:20}           {tn:20}\")\n",
    "    logging.info(\"\\nPerformance Metrics:\")\n",
    "    logging.info(f\"Recall:    {recall:.4f}\")\n",
    "    logging.info(f\"Precision: {precision:.4f}\")\n",
    "    logging.info(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    logging.info(f\"F1 Score:  {f1_score:.4f}\")\n",
    "    logging.info(\"\\n\")\n",
    "#Logging Counter + Stem\n",
    "tp, fp, tn, fn, recall, precision, accuracy, f1_score = evaluate_model(\n",
    "    test_data, \n",
    "    test_labels, \n",
    "    likelihoodMS, \n",
    "    math.log(num_positive_tweets / total_train), \n",
    "    math.log(num_negative_tweets / total_train), \n",
    "    total_vocab_stemmed, \n",
    "    num_positive_tweets, \n",
    "    num_negative_tweets, \n",
    "    use_stemming=True\n",
    ")\n",
    "log_results(tp, fp, tn, fn, recall, precision, accuracy, f1_score, 'CounterVector with Stemming')\n",
    "\n",
    "#Logging Count without Stem\n",
    "tp, fp, tn, fn, recall, precision, accuracy, f1_score = evaluate_model(\n",
    "    test_data, \n",
    "    test_labels, \n",
    "    likelihoodMN, \n",
    "    math.log(num_positive_tweets / total_train), \n",
    "    math.log(num_negative_tweets / total_train), \n",
    "    total_vocab_nostem, \n",
    "    num_positive_tweets, \n",
    "    num_negative_tweets, \n",
    "    use_stemming=False\n",
    ")\n",
    "log_results(tp, fp, tn, fn, recall, precision, accuracy, f1_score, 'CounterVector without Stemming')\n",
    "\n",
    "#Logging Binary with Stem\n",
    "tp, fp, tn, fn, recall, precision, accuracy, f1_score = evaluate_model(\n",
    "    test_data, \n",
    "    test_labels, \n",
    "    likelihoodBS, \n",
    "    math.log(num_positive_tweets / total_train), \n",
    "    math.log(num_negative_tweets / total_train), \n",
    "    total_vocab_stemmed, \n",
    "    num_positive_tweets, \n",
    "    num_negative_tweets, \n",
    "    use_stemming=True\n",
    ")\n",
    "log_results(tp, fp, tn, fn, recall, precision, accuracy, f1_score, 'BinaryVector with Stemming')\n",
    "\n",
    "#Logging Binary without Stem\n",
    "tp, fp, tn, fn, recall, precision, accuracy, f1_score = evaluate_model(\n",
    "    test_data, \n",
    "    test_labels, \n",
    "    likelihoodBN, \n",
    "    math.log(num_positive_tweets / total_train), \n",
    "    math.log(num_negative_tweets / total_train), \n",
    "    total_vocab_nostem, \n",
    "    num_positive_tweets, \n",
    "    num_negative_tweets, \n",
    "    use_stemming=False\n",
    ")\n",
    "log_results(tp, fp, tn, fn, recall, precision, accuracy, f1_score, 'BinaryVector without Stemming')\n",
    "\n",
    "#print results to the console\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"                  Predicted Positive   Predicted Negative\")\n",
    "print(f\"Actual Positive   {tp:20}           {fn:20}\")\n",
    "print(f\"Actual Negative   {fp:20}           {tn:20}\")\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87415d26-8eab-49f4-9b91-6ed2c73acc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
